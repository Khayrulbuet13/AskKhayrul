{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neo4j+s://39ad3c3f.databases.neo4j.io\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "test = os.getenv(\"NEO4J_URI\")\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created successfully.\n"
     ]
    }
   ],
   "source": [
    "!python chatbot_api/src/generate_vector_store.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw retrieval results:\n",
      "SCORE: 0.3464084565639496, METADATA: {'page': 0, 'source': 'chatbot_api/pdfs/12.pdf'}, CONTENT: of precision and analytical depth19–25. The majority of machine\n",
      "learning-based on cell classificatio...\n",
      "SCORE: 0.35743772983551025, METADATA: {'page': 8, 'source': 'chatbot_api/pdfs/12.pdf'}, CONTENT: in comparison to standalone implementations of the CNN or NN\n",
      "models. This improvement underscores th...\n",
      "SCORE: 0.36846837401390076, METADATA: {'page': 4, 'source': 'chatbot_api/pdfs/12.pdf'}, CONTENT: of several classification models, specifically Logistic Regression\n",
      "(LR), Support Vector Machine (SVM...\n",
      "SCORE: 0.3690042495727539, METADATA: {'page': 12, 'source': 'chatbot_api/pdfs/9.pdf'}, CONTENT: 33. Kourou K, Exarchos TP , Exarchos KP , Karamouzis MV, Fotiadis DI. Machine \n",
      "learning applications...\n",
      "SCORE: 0.3694433867931366, METADATA: {'page': 7, 'source': 'chatbot_api/pdfs/12.pdf'}, CONTENT: plication in machine learning where models tailor themselves too\n",
      "closely to the training data, compr...\n",
      "Filtered documents: 0\n"
     ]
    }
   ],
   "source": [
    "# test_vector_retrieval.py (Run this locally)\n",
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embed = OpenAIEmbeddings()\n",
    "vector_store = Chroma(persist_directory=\"./vector_store\", embedding_function=embed)\n",
    "\n",
    "query = \"Explain the methodology of the paper related to machine learning.\"\n",
    "results = vector_store.similarity_search_with_score(query, k=5)\n",
    "\n",
    "print(\"Raw retrieval results:\")\n",
    "for doc, score in results:\n",
    "    print(f\"SCORE: {score}, METADATA: {doc.metadata}, CONTENT: {doc.page_content[:100]}...\")\n",
    "\n",
    "# Check if you're applying threshold correctly\n",
    "threshold = 0.5\n",
    "filtered_docs = [d for d, s in results if s > threshold]  # If you're assuming higher=better\n",
    "print(\"Filtered documents:\", len(filtered_docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug the RetrievalQA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9d/347q2rj132jbbp3kpczb26wm0000gn/T/ipykernel_62154/471344619.py:21: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = papers_qa_chain({\"query\": query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers QA chain result:\n",
      "Answer: The main contribution of the paper titled 'Multiplex Image Machine Learning' is the development of a novel machine learning architecture called the Multiplex Image Machine Learning (MIML) Architecture. This architecture enhances cell classification by integrating label-free cell images with biomechanical property data, allowing for a more comprehensive understanding of cellular properties. The MIML model achieves a remarkable accuracy of 98.3% in cell classification, addressing limitations in specificity and speed found in existing techniques. This advancement has the potential to transform cell biology and biomedical imaging by enabling less invasive studies and new breakthroughs in cell classification.\n",
      "Source documents: [Document(metadata={'page': 7, 'source': 'chatbot_api/pdfs/12.pdf'}, page_content='2.4 Multiplex Image machine learning for cell detection\\nTo enhance the accuracy of cell classification, we have developed\\na novel architectural model, aptly termed the Multiplex Image\\nMachine Learning (MIML) Architecture. This cutting-edge model\\nis capable of processing both image and text data concurrently,\\nenabling it to predict cell types based on a more comprehensive\\nset of input data. The MIML Architecture’s underlying mechan-'), Document(metadata={'page': 0, 'source': 'chatbot_api/pdfs/12.pdf'}, page_content='MIML: Multiplex Image Machine Learning for High Pre-\\ncisionCellClassificationviaMechanicalTraitswithinMi-\\ncrofluidic Systems†\\nKhayrul Islam,aRatul Paul,aShen Wang,aand Yaling Liu∗a,b\\nLabel-free cell classification is advantageous for supplying pristine cells for further use or examina-\\ntion, yet existing techniques frequently fall short in terms of specificity and speed. In this study,\\nwe address these limitations through the development of a novel machine learning framework, Mul-'), Document(metadata={'page': 0, 'source': 'chatbot_api/pdfs/12.pdf'}, page_content='this need has the potential to transform our understanding of cel-\\nlular dynamics. An advanced, label-free classification could sig-\\nnificantly advance cell biology and enhance biomedical imaging,\\npaving the way for less invasive studies and new breakthroughs\\nin cell classification.\\nIn our endeavor to refine cell classification methodologies, we\\ndeveloped a novel machine learning architecture called Multiplex\\nImage Machine Learning (MIML). This architecture is specially'), Document(metadata={'page': 0, 'source': 'chatbot_api/pdfs/12.pdf'}, page_content='tiplex Image Machine Learning (MIML). This architecture uniquely combines label-free cell images\\nwith biomechanical property data, harnessing the vast, often underutilized morphological informa-\\ntion intrinsic to each cell. By integrating both types of data, our model offers a more holistic\\nunderstanding of the cellular properties, utilizing morphological information typically discarded in\\ntraditional machine learning models. This approach has led to a remarkable 98.3% accuracy in cell')]\n"
     ]
    }
   ],
   "source": [
    "# test_papers_qa_chain.py (Run this locally)\n",
    "import os\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "model_name = os.getenv(\"AGENT_MODEL\", \"gpt-3.5-turbo\")\n",
    "chat_model = ChatOpenAI(model=model_name, temperature=0)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = Chroma(persist_directory=\"./vector_store\", embedding_function=embeddings)\n",
    "\n",
    "papers_qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat_model,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector_store.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "query = \"What is the main contribution of the paper titled 'Multiplex Image Machine Learning'?\"\n",
    "result = papers_qa_chain({\"query\": query})\n",
    "\n",
    "print(\"Papers QA chain result:\")\n",
    "print(\"Answer:\", result.get('result', 'No result'))\n",
    "print(\"Source documents:\", result.get('source_documents', []))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Metadata Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In generate_vector_store.py, after you fetch metadata:\n",
    "\n",
    "async def fetch_metadata(self):\n",
    "    with self.driver.session(database=\"neo4j\") as session:\n",
    "        query = \"\"\"\n",
    "        MATCH (p:Paper)\n",
    "        OPTIONAL MATCH (p)-[:UTILIZES]->(s:Skill)\n",
    "        WITH p, collect(s.skill) AS skills\n",
    "        RETURN p.id AS id, properties(p) AS properties, skills\n",
    "        \"\"\"\n",
    "        result = session.run(query)\n",
    "        id_to_metadata = {}\n",
    "        for record in result:\n",
    "            properties = record[\"properties\"]\n",
    "            skills_str = ', '.join(record[\"skills\"]) if record[\"skills\"] else \"No skills listed\"\n",
    "            properties[\"skills\"] = skills_str\n",
    "            # IMPORTANT: Actually store the metadata in the dictionary\n",
    "            paper_id = record[\"id\"]\n",
    "            id_to_metadata[paper_id] = properties\n",
    "\n",
    "        print(\"Fetched Metadata:\", id_to_metadata)  # Debug print\n",
    "        return id_to_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After splitting text_chunks\n",
    "for doc in text_chunks:\n",
    "    source_path = doc.metadata.get('source', '')\n",
    "    filename = os.path.basename(source_path)\n",
    "    file_id, _ = os.path.splitext(filename)\n",
    "    json_meta = id_to_metadata.get(file_id, {})\n",
    "    doc.metadata.update(json_meta)\n",
    "\n",
    "print(\"Sample doc metadata after update:\", text_chunks[0].metadata if text_chunks else \"No documents\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "askkhayrul",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
